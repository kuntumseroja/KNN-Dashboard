Let’s put KNN into a **very concrete banking case** and show how you’d do it in Python:

> “Given all accounts, find which accounts are *behaviorally related* to each other.”

I’ll cover:

1. **Banking scenario & logic**
2. **Feature design (what data we use)**
3. **Python code with KNN**
4. **Libraries you’d typically use**
5. **How an AI agent would sit on top of this**

---

## 1. Banking scenario & logic

Goal:
Identify **relationships between accounts** that are not only explicit (same customer ID) but also **implicit**:

* Same household / same business group
* Possible mule / fraud network
* Similar transaction behavior for cross-sell

**Logic (high-level):**

1. Each **account** becomes a **vector of features**.
2. Use **KNN** to find, for each account, its **K most similar accounts**.
3. Turn those “neighbors” into **relationship edges**:

   * `account_A` ↔ `account_B` with a similarity score.
4. Use those edges to:

   * Detect clusters (communities → households, groups, fraud rings, etc.).
   * Generate recommendations (“accounts like this usually take product X”).

---

## 2. Feature design – what goes into the vector?

Example: one row per account with aggregated features:

```text
account_id
avg_txn_amount_30d
txn_count_30d
salary_inflow_90d
cash_withdrawal_ratio_90d
merchant_diversity_90d
unique_devices_90d
unique_ips_90d
country_risk_score
segment_code (e.g. Retail / SME / Corporate)
channel_mix_online_ratio
```

**Logic:**

* Numeric features → used directly (after scaling).
* Categorical (segment_code) → one-hot encode.
* If you have text (e.g., account_notes, employer_name) → turn into **embeddings** and append.

---

## 3. Python code – KNN-based account relationship graph

### 3.1. Imports and data prep

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.neighbors import NearestNeighbors
import numpy as np
import networkx as nx
```

Assume you already have a `DataFrame` like:

```python
# Example schema:
# account_id, avg_txn_amount_30d, txn_count_30d, salary_inflow_90d, 
# cash_withdrawal_ratio_90d, merchant_diversity_90d, unique_devices_90d,
# unique_ips_90d, country_risk_score, segment_code, channel_mix_online_ratio

df = pd.read_parquet("account_features.parquet")  # or CSV/SQL etc.
account_ids = df["account_id"].values
```

Define which columns are numeric vs categorical:

```python
numeric_features = [
    "avg_txn_amount_30d",
    "txn_count_30d",
    "salary_inflow_90d",
    "cash_withdrawal_ratio_90d",
    "merchant_diversity_90d",
    "unique_devices_90d",
    "unique_ips_90d",
    "country_risk_score",
    "channel_mix_online_ratio",
]

categorical_features = ["segment_code"]
```

### 3.2. Preprocessing + KNN pipeline

We’ll **scale** numeric features and **one-hot encode** segment:

```python
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown="ignore")

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ]
)

# We’ll use KNN for similarity search
knn = NearestNeighbors(
    n_neighbors=6,      # 1 self + 5 neighbors
    metric="euclidean"  # or "cosine" if you use embeddings
)

# Full pipeline: preprocess → KNN fit
pipeline = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("knn", knn)
])

X = df[numeric_features + categorical_features]
pipeline.fit(X)

# After fit, we can access the preprocessed matrix for custom logic if needed:
X_processed = pipeline.named_steps["preprocess"].transform(X)
```

### 3.3. Find neighbors and build relation edges

```python
# Get neighbors: distances and indices
distances, indices = pipeline.named_steps["knn"].kneighbors(X_processed)

edges = []
max_distance_for_strong_link = 1.5  # domain-tuned threshold

for i, (d_row, idx_row) in enumerate(zip(distances, indices)):
    src_acc = account_ids[i]
    
    # First neighbor is the account itself; skip index 0
    for dist, j in zip(d_row[1:], idx_row[1:]):
        dst_acc = account_ids[j]
        
        # Filter by distance threshold
        if dist <= max_distance_for_strong_link:
            similarity = float(np.exp(-dist))  # simple transform → higher = more similar
            edges.append((src_acc, dst_acc, {"distance": float(dist),
                                             "similarity": similarity}))
```

Now `edges` = **relationship list**:

```python
# Example row:
# ("ACC123", "ACC987", {"distance": 0.8, "similarity": 0.449})
```

You can save it to a table:

```python
relations_df = pd.DataFrame([
    {"src": s, "dst": d, "distance": attr["distance"], "similarity": attr["similarity"]}
    for s, d, attr in edges
])
```

Or build a graph:

```python
G = nx.Graph()
G.add_nodes_from(account_ids)
G.add_edges_from(edges)
```

Then you can:

* Detect communities: `nx.community.louvain_communities(G)` (if you have the extra package)
* Pull neighbors for a given account: `G.neighbors("ACC123")`
* Compute centrality: `nx.degree_centrality(G)` → which accounts are “hubs” (typical for mule accounts)

---

## 4. Libraries you’d typically use

**Core:**

1. `pandas` – load & manipulate transaction/feature tables.
2. `scikit-learn`:

   * `StandardScaler`, `OneHotEncoder`, `ColumnTransformer`, `Pipeline`
   * `NearestNeighbors` for KNN search
3. `numpy` – numeric operations.
4. `networkx` – create **account relationship graphs** (nodes = accounts, edges = similar accounts).

**Optional (for larger/AI-heavy setups):**

* **Vector DB / ANN libs**:

  * `faiss`, `annoy`, `hnswlib` – fast approximate KNN for millions of accounts.
* **Text embeddings**:

  * `sentence-transformers` (for employer names, descriptions, free-text KYC fields)
  * Then use cosine similarity with KNN or directly within vector DB.

---

## 5. How an AI “Agent” would use this

You can wrap all the above into an **Account Relationship Agent**:

1. **Data Agent**

   * Pulls features from DWH / feature store.
2. **Embedding/Feature Agent**

   * Applies preprocessing / embeddings.
3. **KNN Agent**

   * Runs `NearestNeighbors` (or vector DB search) for a given account.
4. **Reasoning Agent (LLM)**

   * Takes the list of neighbors + feature values.
   * Explains:

     > “Account 123 is closely related to 456 and 789 because they have similar salary inflow patterns, channel usage, and device fingerprints over the last 90 days.”
5. **Action Agent**

   * Uses these relations for:

     * Cross-sell (similar profitable accounts)
     * Risk/Fraud (dense suspicious clusters)
     * Household/group-level limits or pricing

---

If you’d like, next step I can:

* Adapt this to a **fraud-ring detection** example, or
* Show how to plug this into a **vector database + agentic AI** pattern (e.g., tools + LLM) for interactive “Explain this customer’s network” queries.
